{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forex Market Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "conf = SparkConf().setAppName(\"MLlib\")\n",
    "#sc.stop() #if need to \n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training data\n",
    "train_df = spark.read.format(\"csv\").option(\"header\", \"true\"). \\\n",
    "            load(r'Train_small.csv')\n",
    "train_cols = train_df.columns\n",
    "\n",
    "#Remove Gmt time column from the data and convert the values to float.\n",
    "train_cols.remove('Gmt time')\n",
    "train_df = train_df.select(train_cols).rdd.map(lambda x: [float(i) for i in x]).toDF(train_cols)\n",
    "\n",
    "#Load test data\n",
    "test_df = spark.read.format(\"csv\").option(\"header\", \"true\"). \\\n",
    "            load(r'Test_small_feature.csv')\n",
    "test_cols = test_df.columns\n",
    "\n",
    "#Remove Gmt time and unnamed index column and convert values in the test dataset to float\n",
    "test_cols.remove('Gmt time')\n",
    "test_cols.remove('_c0')\n",
    "test_df = test_df.select(test_cols).rdd.map(lambda x: [float(i) for i in x]).toDF(test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classification with MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 17307\n",
      "Test Dataset Count: 4295\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,8,9...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,8,9...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,8,9...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,8,9...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,8,9...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,8,9...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,8,9...|\n",
      "|       0.0|  0.0|(214,[0,1,2,3,8,9...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Error = 0 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Create a dense vector of features\n",
    "stages = []\n",
    "label_stringIdx = StringIndexer(inputCol = 'up_down', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "assembler = VectorAssembler(inputCols=train_cols, outputCol=\"features\", )\n",
    "stages += [assembler]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(train_df)\n",
    "df = pipelineModel.transform(train_df)\n",
    "selectedCols = ['label', 'features'] + train_cols\n",
    "df = df.select(selectedCols)\n",
    "\n",
    "#Split the data into train and test\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 0)\n",
    "\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n",
    "\n",
    "#Using a Decision Tree classifier model to fit the trainig data\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "model_dt = dt.fit(train)\n",
    "\n",
    "#Predict up_down for test data\n",
    "predictions = model_dt.transform(test)\n",
    "\n",
    "#Test accuracy using MulticlassClassificationEvaluator\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precict up_down for test data using Decision Tree Classifier\n",
    "\n",
    "#Create a dense vector of features \n",
    "stages = []\n",
    "assembler = VectorAssembler(inputCols=test_cols, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(test_df)\n",
    "test_data = pipelineModel.transform(test_df)\n",
    "selectedCols = ['features'] + test_cols\n",
    "test_data = test_data.select(selectedCols)\n",
    "\n",
    "#Test the model to get the up_down predictions\n",
    "test_pred_dt = model_dt.transform(test_data)\n",
    "\n",
    "#Select the prediction column and rename it to uo_down, convert the data frame to pandas\n",
    "#and copy them into a csv file \n",
    "test_pred_dt.select('prediction').withColumnRenamed('prediction', 'up_down').\\\n",
    "    toPandas().to_csv('Test_pred2_dc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions accuracy = 0.962776, Test Error = 0.0372243\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = train_df.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).\\\n",
    "           toDF(['label','features'])\n",
    "\n",
    "(trainingData, testData) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "#Set layers\n",
    "layers = [len(train_cols)-1, len(train_cols), 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=250, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model using trainingData\n",
    "model_mlp = mlp.fit(trainingData)\n",
    "\n",
    "#precit the up_down for testData\n",
    "predictions = model_mlp.transform(testData)\n",
    "\n",
    "#Test accuracy using MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Predictions accuracy = %g, Test Error = %g\" % (accuracy,(1.0 - accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precict up_down for test data using MLP Classifier\n",
    "\n",
    "test_data = test_df.rdd.map(lambda r: [Vectors.dense(r[:])]).toDF(['features'])\n",
    "test_preds_mlp = model_mlp.transform(test_data)\n",
    "test_preds_mlp.select('prediction').withColumnRenamed('prediction', 'up_down').toPandas().to_csv('Test_pred2_mlp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.37061383205258774\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "#Create a Labeled point dataframe\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line]\n",
    "    return LabeledPoint(values[213], values[0:213])\n",
    "\n",
    "parsedData = train_df.rdd.map(parsePoint)\n",
    "\n",
    "#Use SVM with SGD classifier to train the model\n",
    "model_svm = SVMWithSGD.train(parsedData, iterations=10000)\n",
    "\n",
    "#Predict the up_down\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model_svm.predict(p.features)))\n",
    "\n",
    "#Filter the data in predictions with those that match the test data \n",
    "#and calculate the error\n",
    "trainErr = labelsAndPreds.filter(lambda x: x[0] != x[1]).count() / float(train_df.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precict up_down for test data using SVM with SGD\n",
    "\n",
    "test_data = test_df.rdd.map(lambda x:[i for i in x])\n",
    "test_pred_svm = model_svm.predict(test_data)\n",
    "pred_svm = test_pred_svm.collect()\n",
    "data = pd.DataFrame({'up_down':pred_svm})\n",
    "filepath = 'Test_pred2_svm.csv'\n",
    "data.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
